export default `
Transformers were the breakthrough machine learning architecture that led to the rapid advancement of language models (and language to image models) in the early 2020s.
First detailed in the "Attention Is All You Need" paper, transformers added multi-head attention layers to a RNN-like architecture, dramatically improving performance.
The attention metaphor is revolutionary to me, and it has become the basis of my thought on a manner of different subjects from neurology to metaphysics.`;
